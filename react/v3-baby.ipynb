{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "babyAGI from scratch\n",
    "\n",
    "source:\n",
    "1. https://community.openai.com/t/building-agent-from-scratch/240703/6\n",
    "2. https://github.com/Troyanovsky/autonomous_agent_tutorial/blob/main/autonomous_agent_handson.ipynb\n",
    "  * https://bootcamp.uxdesign.cc/a-comprehensive-and-hands-on-guide-to-autonomous-agents-with-gpt-b58d54724d50\n",
    "\n",
    "TODO:\n",
    "1. Create/use an API replay tool. Make it easy to replay API calls as it’s too slow to copy & paste these into the OpenAI playground.\n",
    "2. Stream completions. Use the stream mode of the API to speed up dev cycles. You can quickly abort if a completion is off the rails.\n",
    "\n",
    "Extensions:\n",
    "* https://twitter.com/yoheinakajima/status/1666313838868992001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../creds/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "azure_endpoint = \"https://cursor-gpt-4.openai.azure.com\"\n",
    "api_version=\"2024-02-15-preview\"\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=api_version,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****OBJECTIVE*****\n",
      "Become a machine learning expert.\n",
      "****Expounding based on task:**** Learn about tensors.\n",
      "#### (1) Generated Task ####\n",
      "****Expounding based on task:**** Research and write a summary of the different data types that tensors can store in machine learning, including their significance and use cases.\n",
      "In machine learning, tensors are multi-dimensional arrays that can store various types of data. Tensors are the fundamental building blocks of most machine learning frameworks, and they can store different types of data, including scalar, vector, and matrix data.\n",
      "\n",
      "1. Scalar Data:\n",
      "Scalars are single numerical values, such as integers or floating-point numbers, and they are represented as tensors with zero dimensions. Scalar tensors are commonly used to store constants or individual data points in machine learning algorithms. For example, scalar tensors can be used to represent bias values in neural networks or as individual data points in statistical analysis.\n",
      "\n",
      "2. Vector Data:\n",
      "Vectors are one-dimensional arrays of numerical values and are represented as tensors with one dimension. In machine learning, vectors are commonly used to store feature vectors, which represent input data for training models. For example, in image recognition tasks, each image can be represented as a vector of pixel values stored in a tensor. Vectors are also used to represent output labels in classification tasks.\n",
      "\n",
      "3. Matrix Data:\n",
      "Matrices are two-dimensional arrays of numerical values and are represented as tensors with two dimensions. Matrices are widely used in machine learning for tasks such as linear algebra operations, image processing, and natural language processing. For example, in natural language processing, word embeddings are often represented as matrices stored in tensors, where each row corresponds to a word and each column corresponds to a feature dimension.\n",
      "\n",
      "4. Higher-Dimensional Data:\n",
      "Tensors can also store higher-dimensional data, such as three-dimensional or higher-dimensional arrays. These higher-dimensional tensors are commonly used in tasks such as image processing, video analysis, and time-series data. For example, in image processing, a color image can be represented as a three-dimensional tensor, where the dimensions correspond to the height, width, and color channels of the image.\n",
      "\n",
      "Overall, tensors in machine learning can store a wide variety of data types, including scalar, vector, matrix, and higher-dimensional data. Understanding the significance and use cases of these different data types is essential for effectively working with tensors in machine learning applications.\n",
      "#### (2) Generated Task ####\n",
      "****Expounding based on task:**** Create a Python script using TensorFlow or PyTorch to define and manipulate tensors of different ranks and shapes, and perform basic operations such as addition, multiplication, and matrix operations.\n",
      "Sure, I can help you with that. Below is an example of a Python script using TensorFlow to define and manipulate tensors of different ranks and shapes, and perform basic operations such as addition, multiplication, and matrix operations.\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "\n",
      "# Define tensors of different ranks and shapes\n",
      "scalar = tf.constant(5)  # a rank 0 tensor (or scalar)\n",
      "vector = tf.constant([1, 2, 3, 4])  # a rank 1 tensor (or vector)\n",
      "matrix = tf.constant([[1, 2], [3, 4]])  # a rank 2 tensor (or matrix)\n",
      "cube = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # a rank 3 tensor (or cube)\n",
      "\n",
      "# Perform basic operations\n",
      "addition = tf.add(matrix, matrix)  # element-wise addition\n",
      "multiplication = tf.multiply(matrix, matrix)  # element-wise multiplication\n",
      "matrix_product = tf.matmul(matrix, matrix)  # matrix multiplication\n",
      "\n",
      "# Print the results\n",
      "with tf.Session() as sess:\n",
      "    print(\"Scalar:\", sess.run(scalar))\n",
      "    print(\"Vector:\", sess.run(vector))\n",
      "    print(\"Matrix:\", sess.run(matrix))\n",
      "    print(\"Cube:\", sess.run(cube))\n",
      "    print(\"Matrix addition:\", sess.run(addition))\n",
      "    print(\"Matrix multiplication:\", sess.run(multiplication))\n",
      "    print(\"Matrix product:\", sess.run(matrix_product))\n",
      "```\n",
      "\n",
      "You can run this script to see how tensors of different ranks and shapes are defined and manipulated using TensorFlow, and how basic operations such as addition, multiplication, and matrix operations are performed.\n",
      "#### (3) Generated Task ####\n",
      "****Expounding based on task:**** Investigate and summarize the role of tensors in representing input data, model parameters, and output predictions in the context of machine learning, with a focus on how tensors are used in deep learning frameworks.\n",
      "Tensors play a fundamental role in representing input data, model parameters, and output predictions in the context of machine learning. In the context of deep learning frameworks, tensors are used extensively due to their ability to efficiently represent and manipulate multi-dimensional data.\n",
      "\n",
      "Input Data:\n",
      "In machine learning, input data such as images, sound waves, and text are represented as tensors. For example, an RGB image can be represented as a 3D tensor, with dimensions corresponding to height, width, and color channels. Similarly, a sequence of words in natural language processing can be represented as a 2D tensor, with dimensions corresponding to the length of the sequence and the embedding size of each word.\n",
      "\n",
      "Model Parameters:\n",
      "The parameters of a machine learning model, such as weights and biases, are also represented as tensors. These tensors capture the learned information from the input data and are updated during the training process to minimize the loss function.\n",
      "\n",
      "Output Predictions:\n",
      "The output predictions of a machine learning model are also represented as tensors. The dimensions of the output tensor depend on the specific task, such as classification, regression, or sequence generation.\n",
      "\n",
      "Deep Learning Frameworks:\n",
      "Deep learning frameworks, such as TensorFlow and PyTorch, provide efficient and optimized operations for manipulating tensors. These frameworks offer a wide range of tensor operations, including tensor multiplication, addition, convolution, and more. They also provide automatic differentiation, which is crucial for training deep learning models.\n",
      "\n",
      "In summary, tensors are a crucial concept in machine learning, especially in the context of deep learning. They are used to represent input data, model parameters, and output predictions, and deep learning frameworks provide powerful tools for working with tensors efficiently.\n",
      "#### (4) Generated Task ####\n",
      "****Expounding based on task:**** Explore and compare the advantages and disadvantages of using different tensor manipulation operations in machine learning, such as element-wise operations, matrix multiplication, and tensor reshaping.\n",
      "Tensor manipulation operations are crucial in machine learning as they allow us to perform various mathematical operations on the data. Here, I will explore and compare the advantages and disadvantages of using different tensor manipulation operations such as element-wise operations, matrix multiplication, and tensor reshaping.\n",
      "\n",
      "1. Element-wise operations:\n",
      "   Advantages:\n",
      "   - Element-wise operations are simple and efficient, making it easy to perform operations such as addition, subtraction, multiplication, and division on tensors.\n",
      "   - These operations can be parallelized, which can significantly speed up the calculations, especially on hardware with parallel processing capabilities like GPUs.\n",
      "   \n",
      "   Disadvantages:\n",
      "   - Element-wise operations may not fully utilize the computational power of modern hardware, as they do not fully exploit the potential of parallel processing.\n",
      "\n",
      "2. Matrix multiplication:\n",
      "   Advantages:\n",
      "   - Matrix multiplication is a fundamental operation in many machine learning algorithms, such as neural networks. It allows us to combine and transform data in complex ways.\n",
      "   - This operation can be optimized for parallel processing, making it well-suited for modern hardware with multiple cores or GPUs.\n",
      "\n",
      "   Disadvantages:\n",
      "   - Matrix multiplication can be computationally expensive, especially for large matrices, and may become a bottleneck in the performance of machine learning algorithms.\n",
      "   - It requires careful handling of dimensions and shapes to ensure compatibility, which can be error-prone.\n",
      "\n",
      "3. Tensor reshaping:\n",
      "   Advantages:\n",
      "   - Tensor reshaping allows us to reorganize the data without changing its contents, which is useful for preparing the input data for different layers in a neural network or for interfacing with different machine learning libraries.\n",
      "   - It can help in reducing the memory footprint of the data, making it more memory-efficient.\n",
      "\n",
      "   Disadvantages:\n",
      "   - Reshaping tensors requires careful consideration of the new shape and may lead to errors if not done correctly.\n",
      "   - Reshaping can also introduce complexity in the code and make it harder to understand and maintain.\n",
      "\n",
      "In conclusion, each tensor manipulation operation has its advantages and disadvantages. Element-wise operations are simple and efficient but may not fully exploit the computational power of modern hardware. Matrix multiplication is fundamental for many machine learning algorithms but can be computationally expensive. Tensor reshaping is useful for reorganizing data but requires careful handling of shapes and dimensions. It's important to carefully consider the trade-offs and choose the appropriate operations based on the specific requirements of the machine learning task at hand.\n",
      "#### (5) Generated Task ####\n",
      "****Expounding based on task:**** Develop a tutorial on how to implement and optimize a simple machine learning model using tensors in TensorFlow or PyTorch, focusing on the practical application of tensors in model training and inference processes.\n",
      "To implement and optimize a simple machine learning model using tensors in TensorFlow, you can follow the tutorial below. We will walk through the process of creating a simple neural network model, training it, and making predictions using TensorFlow and its high-level Keras API.\n",
      "\n",
      "Step 1: Install TensorFlow\n",
      "First, you need to install TensorFlow. You can do this using pip:\n",
      "\n",
      "```bash\n",
      "pip install tensorflow\n",
      "```\n",
      "\n",
      "Step 2: Import the necessary libraries\n",
      "Once TensorFlow is installed, you can import the necessary libraries in your Python script:\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "from tensorflow import keras\n",
      "import numpy as np\n",
      "```\n",
      "\n",
      "Step 3: Prepare the data\n",
      "Next, you need to prepare the training and testing data for your model. In this example, we'll use a simple dataset of input features and corresponding target labels:\n",
      "\n",
      "```python\n",
      "# Generate dummy data\n",
      "x_train = np.random.random((1000, 3))\n",
      "y_train = np.random.randint(2, size=(1000, 1))\n",
      "x_test = np.random.random((100, 3))\n",
      "y_test = np.random.randint(2, size=(100, 1))\n",
      "```\n",
      "\n",
      "Step 4: Create the model\n",
      "Now, you can create a simple neural network model using TensorFlow's Keras API. We'll define a model with a single hidden layer and an output layer:\n",
      "\n",
      "```python\n",
      "model = keras.Sequential([\n",
      "    keras.layers.Dense(64, input_shape=(3,), activation='relu'),\n",
      "    keras.layers.Dense(1, activation='sigmoid')\n",
      "])\n",
      "```\n",
      "\n",
      "Step 5: Compile the model\n",
      "After creating the model, you need to compile it by specifying the loss function, optimizer, and metrics to be used during training:\n",
      "\n",
      "```python\n",
      "model.compile(optimizer='adam',\n",
      "              loss='binary_crossentropy',\n",
      "              metrics=['accuracy'])\n",
      "```\n",
      "\n",
      "Step 6: Train the model\n",
      "Now, you can train the model using the prepared training data:\n",
      "\n",
      "```python\n",
      "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
      "```\n",
      "\n",
      "Step 7: Make predictions\n",
      "Once the model is trained, you can use it to make predictions on new data:\n",
      "\n",
      "```python\n",
      "predictions = model.predict(x_test)\n",
      "```\n",
      "\n",
      "Step 8: Model evaluation\n",
      "Finally, you can evaluate the model's performance on the test data:\n",
      "\n",
      "```python\n",
      "loss, accuracy = model.evaluate(x_test, y_test)\n",
      "print(f'Test accuracy: {accuracy}')\n",
      "```\n",
      "\n",
      "Optimizing the model:\n",
      "To optimize the model, you can experiment with different hyperparameters such as the number of layers, nodes per layer, learning rate, and batch size. Additionally, you can try different activation functions, regularization techniques, and other advanced optimization algorithms provided by TensorFlow.\n",
      "\n",
      "In conclusion, this tutorial has covered the practical implementation of a simple machine learning model using tensors in TensorFlow. By following these steps, you can gain hands-on experience in building and optimizing machine learning models using TensorFlow's powerful tensor operations and high-level Keras API.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import openai\n",
    "# Goal: Design a simple AI Agent with no dependencies!\n",
    "# This AI will NOT run forever.  It is also safe since it doesn't have API access beyond the OpenAI API.\n",
    "#\n",
    "# Usage: Just set your MainObjective, InitialTask, OPENAI_API_KEY at a minimum.\n",
    "#\n",
    "# Tips: Feel free to play with the temperature and run over and over for different answers.\n",
    "#\n",
    "# Inspired from BabyAGI: https://github.com/yoheinakajima/babyagi\n",
    "# BabyAGI has many more features and bells and whistles.  But may be hard to understand for beginners.\n",
    "\n",
    "# Goal configuration\n",
    "MainObjective = \"Become a machine learning expert.\" # overall objective\n",
    "InitialTask = \"Learn about tensors.\" # first task to research\n",
    "\n",
    "# Note: As expected, GPT-4 gives much deeper answers.  But turbo is selected here as the default, so as there no cost surprises.\n",
    "OPENAI_API_MODEL = \"pjf-dpo-turbo-35\" # use \"gpt-4\" or \"gpt-3.5-turbo\"\n",
    "# deployment_name = \"cursor-gpt-4\"\n",
    "# deployment_name = \"pjf-dpo-turbo-35\"\n",
    "\n",
    "# Model configuration\n",
    "OPENAI_TEMPERATURE = 0.7\n",
    "\n",
    "# Max tokens that the model can output per completion\n",
    "OPENAI_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "# print objective\n",
    "print(\"*****OBJECTIVE*****\")\n",
    "print(f\"{MainObjective}\")\n",
    "\n",
    "\n",
    "# dump task array to string\n",
    "def dumpTask(task):\n",
    "    d = \"\" # init\n",
    "    for tasklet in task:\n",
    "        d += f\"\\n{tasklet.get('task_name','')}\"\n",
    "    d = d.strip()\n",
    "    return d\n",
    "\n",
    "\n",
    "# inference using OpenAI API, with error throws and backoffs\n",
    "def OpenAiInference(\n",
    "    prompt: str,\n",
    "    model: str = OPENAI_API_MODEL,\n",
    "    temperature: float = OPENAI_TEMPERATURE,\n",
    "    max_tokens: int = 1024,\n",
    "):\n",
    "    while True:\n",
    "        try:\n",
    "            # Use chat completion API\n",
    "            response = \"NOTHING\"\n",
    "            messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except openai.error.RateLimitError:\n",
    "            print(\n",
    "                \"   *** The OpenAI API rate limit has been exceeded. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.Timeout:\n",
    "            print(\n",
    "                \"   *** OpenAI API timeout occured. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.APIError:\n",
    "            print(\n",
    "                \"   *** OpenAI API error occured. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.APIConnectionError:\n",
    "            print(\n",
    "                \"   *** OpenAI API connection error occured. Check your network settings, proxy configuration, SSL certificates, or firewall rules. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.InvalidRequestError:\n",
    "            print(\n",
    "                \"   *** OpenAI API invalid request. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.ServiceUnavailableError:\n",
    "            print(\n",
    "                \"   *** OpenAI API service unavailable. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        finally:\n",
    "            pass\n",
    "            # print(f\"Inference Response: {response}\")\n",
    "\n",
    "# expound on the main objective given a task\n",
    "def ExpoundTask(MainObjective: str, CurrentTask: str):\n",
    "\n",
    "    print(f\"****Expounding based on task:**** {CurrentTask}\")\n",
    "\n",
    "    prompt=(f\"You are an AI who performs one task based on the following objective: {MainObjective}\\n\"\n",
    "            f\"Your task: {CurrentTask}\\nResponse:\")\n",
    "\n",
    "\n",
    "    # print(\"################\")\n",
    "    # print(prompt)\n",
    "    response = OpenAiInference(prompt, OPENAI_API_MODEL, OPENAI_TEMPERATURE, OPENAI_MAX_TOKENS)\n",
    "    new_tasks = response.split(\"\\n\") if \"\\n\" in response else [response]\n",
    "    return [{\"task_name\": task_name} for task_name in new_tasks]\n",
    "\n",
    "\n",
    "\n",
    "# generate a bunch of tasks based on the main objective and the current task\n",
    "def GenerateTasks(MainObjective: str, TaskExpansion: str):\n",
    "    prompt=(f\"You are an AI who creates tasks based on the following MAIN OBJECTIVE: {MainObjective}\\n\"\n",
    "            f\"Create tasks pertaining directly to your previous research here:\\n\"\n",
    "            f\"{TaskExpansion}\\nResponse:\")\n",
    "    response = OpenAiInference(prompt, OPENAI_API_MODEL, OPENAI_TEMPERATURE, OPENAI_MAX_TOKENS)\n",
    "    new_tasks = response.split(\"\\n\") if \"\\n\" in response else [response]\n",
    "    task_list = [{\"task_name\": task_name} for task_name in new_tasks]\n",
    "    new_tasks_list = []\n",
    "    for task_item in task_list:\n",
    "        # print(task_item)\n",
    "        task_description = task_item.get(\"task_name\")\n",
    "        if task_description:\n",
    "            # print(task_description)\n",
    "            task_parts = task_description.strip().split(\".\", 1)\n",
    "            # print(task_parts)\n",
    "            if len(task_parts) == 2:\n",
    "                new_task = task_parts[1].strip()\n",
    "                new_tasks_list.append(new_task)\n",
    "\n",
    "    return new_tasks_list\n",
    "\n",
    "# Simple version here, just generate tasks based on the inital task and objective, then expound with GPT against the main objective and the newly generated tasks.\n",
    "q = ExpoundTask(MainObjective,InitialTask)\n",
    "ExpoundedInitialTask = dumpTask(q)\n",
    "\n",
    "q = GenerateTasks(MainObjective, ExpoundedInitialTask)\n",
    "\n",
    "TaskCounter = 0\n",
    "for Task in q:\n",
    "    TaskCounter += 1\n",
    "    print(f\"#### ({TaskCounter}) Generated Task ####\")\n",
    "    e = ExpoundTask(MainObjective,Task)\n",
    "    print(dumpTask(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser==6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: requests==2.31.0 in /Users/ogb/projects/cal/venv/lib/python3.10/site-packages (from arxiv) (2.31.0)\n",
      "Collecting sgmllib3k (from feedparser==6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/ogb/projects/cal/venv/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ogb/projects/cal/venv/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ogb/projects/cal/venv/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ogb/projects/cal/venv/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (2023.5.7)\n",
      "Downloading arxiv-2.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=5b95e83d7a3adb82fbc9bde5f30937b0f0b190dfab4b2853a7cd69788bdfd53b\n",
      "  Stored in directory: /Users/ogb/Library/Caches/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.0 feedparser-6.0.10 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RESPONSE--- USE : searchArxiv('react')\n",
      "parsed string ['USE ', \" searchArxiv('react')\"]\n",
      "Tool Name: searchArxiv\n",
      "Parameter: react\n",
      "THOUGHT: USE : searchArxiv('react')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qp/hpk75cps1fxdym0m5bmqczz00000gn/T/ipykernel_86129/1417250408.py:132: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBSERVATION: [('title: On the road to percent accuracy IV: ReACT -- computing the non-linear power spectrum beyond $Λ$CDM', 'published_date: 2020-05-25', 'authors: Benjamin Bose, Matteo Cataneo, Tilman Tröster, Qianli Xia, Catherine Heymans, Lucas Lombriser', \"summary: To effectively exploit large-scale structure surveys, we depend on accurate\\nand reliable predictions of non-linear cosmological structure formation. Tools\\nfor efficient and comprehensive computational modelling are therefore essential\\nto perform cosmological parameter inference analyses. We present the public\\nsoftware package ReACT, demonstrating its capability for the fast and accurate\\ncalculation of non-linear power spectra from non-standard physics. We showcase\\nReACT through a series of analyses on the DGP and $f(R)$ gravity models,\\nadopting LSST-like cosmic shear power spectra. Accurate non-linear modelling\\nwith ReACT has the potential to more than double LSST's constraining power on\\nthe $f(R)$ parameter, in contrast to an analysis that is limited to the\\nquasi-linear regime. We find that ReACT is sufficiently robust for the\\ninference of consistent constraints on theories beyond $\\\\Lambda$CDM for current\\nand ongoing surveys. With further improvement, particularly in terms of the\\naccuracy of the non-linear $\\\\Lambda$CDM power spectrum, ReACT can, in\\nprinciple, meet the accuracy requirements for future surveys such as Euclid and\\nLSST.\"), ('title: ReAct! An Interactive Tool for Hybrid Planning in Robotics', 'published_date: 2013-07-29', 'authors: Zeynep Dogmus, Esra Erdem, Volkan Patoglu', \"summary: We present ReAct!, an interactive tool for high-level reasoning for cognitive\\nrobotic applications. ReAct! enables robotic researchers to describe robots'\\nactions and change in dynamic domains, without having to know about the\\nsyntactic and semantic details of the underlying formalism in advance, and\\nsolve planning problems using state-of-the-art automated reasoners, without\\nhaving to learn about their input/output language or usage. In particular,\\nReAct! can be used to represent sophisticated dynamic domains that feature\\nconcurrency, indirect effects of actions, and state/transition constraints. It\\nallows for embedding externally defined calculations (e.g., checking for\\ncollision-free continuous trajectories) into representations of hybrid domains\\nthat require a tight integration of (discrete) high-level reasoning with\\n(continuous) geometric reasoning. ReAct! also enables users to solve planning\\nproblems that involve complex goals. Such variety of utilities are useful for\\nrobotic researchers to work on interesting and challenging domains, ranging\\nfrom service robotics to cognitive factories. ReAct! provides sample\\nformalizations of some action domains (e.g., multi-agent path planning, Tower\\nof Hanoi), as well as dynamic simulations of plans computed by a\\nstate-of-the-art automated reasoner (e.g., a SAT solver or an ASP solver).\"), ('title: MVP-Workshop Contribution: Modeling of Volvo Bluff Flame Experiment and Comparison of Finite-Volume and Discontinuous-Galerkin Schemes', 'published_date: 2017-07-18', 'authors: Hao Wu, Peter C. Ma, Yu Lv, Matthias Ihme', 'summary: The Volvo burner features the canonical configuration of a bluff-body\\nstabilized premixed flame. This configuration was studied experimentally under\\nthe Volvo Flygmotor AB program. Two cases are considered in this study: a\\nnon-reacting case with an inlet flow speed of 16.6 m/s and a reacting case with\\nequilibrium ratio of 0.65 and inflow speed of 17.3 m/s. The characteristic\\nvortex shedding in the wake behind the bluff body is present in the\\nnon-reacting case, while two oscillation modes are intermittently present in\\nthe reacting case. A series of large-eddy simulations are performed on this\\nconfiguration using two solvers, one using a high-resolution finite-volume (FV)\\nscheme and the other featuring a high-order discontinuous-Galerkin (DG)\\ndiscretization. The FV calculations are conducted on hexahedral meshes with\\nthree different resolution (4mm, 2mm, and 1mm). The DG calculations are\\nperformed using two different polynomial orders on the same tetrahedral mesh.\\nFor the non-reacting cases, good agreement with respect to the experimental\\ndata is achieved by both solvers at high numerical resolution. The reacting\\ncases are calculated using a two-step global mechanism in combination with the\\nthickened-flame model. Reasonable agreement with experiments is obtained by\\nboth solvers at higher resolution. Models for combustion-turbulence interaction\\nare necessary for the reacting case as it contains the length scale of the\\nflame, which is smaller than the grid resolution in all calculations. The\\nimpact of such models on the flame stability and flow/flame dynamics is the\\nsubject of future research.')]\n",
      "---RESPONSE--- FINAL ANSWER: React is a JavaScript library for building user interfaces, particularly for single page applications. It's used for handling view layer for web and mobile apps. React allows you to design simple views for each state in your application, and React will efficiently update and render just the right components when your data changes.\n",
      "FINAL ANSWER: React is a JavaScript library for building user interfaces, particularly for single page applications. It's used for handling view layer for web and mobile apps. React allows you to design simple views for each state in your application, and React will efficiently update and render just the right components when your data changes.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import arxiv\n",
    "\n",
    "\"\"\"\n",
    "Wrap the OpenAI API call in this function\n",
    "\"\"\"\n",
    "def getResponse(prompt):\n",
    "    response =  client.chat.completions.create(\n",
    "            # model=\"pjf-dpo-turbo-35\",\n",
    "            model=\"cursor-gpt-4\",\n",
    "            temperature = 0, # We want consistent behavior, so we set a very low temperature\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You're a helpful assistant. Carefully follow the user's instructions.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # response = response['choices'][0]['message']['content']\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\"\"\"\n",
    "Use GPT to determine the action to take by giving it the objective, memory, and tools.\n",
    "If it think it has finished the objective, just give the answer.\n",
    "If it needs more info, it will pick the tool to get the relevant information based on the tool description.\n",
    "\"\"\"\n",
    "def determineAction(objective, memory, tools):\n",
    "    formattedPrompt = f\"\"\"Determine if the following memory is enough to answer\\n\n",
    "    the user's objective. Your past actions are stored in the memory for reference\\n\n",
    "    If it is enough, answer the question in the format: 'FINAL ANSWER: '. \\n\n",
    "    If the memory is not enough, you can use a tool in the available tools section\\n\n",
    "    to get more information. When using a tool you should use this format: \\n\n",
    "    'USE :'. If no tool can help you achieve the user's \\n\n",
    "    objective, then answer 'FINAL: CANNOT ANSWER'.\n",
    "\n",
    "    ```Objective\n",
    "    Answer: {objective}\n",
    "    ```\n",
    "\n",
    "    ```Memory\n",
    "    {memory}\n",
    "    ```\n",
    "\n",
    "    ```Available Tools\n",
    "    {tools}\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    response = getResponse(formattedPrompt)\n",
    "    (finished, result, memory) = parseResponse(response, memory,tools)\n",
    "    return (finished, result, memory)\n",
    "\n",
    "\"\"\"\n",
    "Parse the response from GPT to determine if the objective is finished.\n",
    "If it is finished, just give the final answer.\n",
    "If the objective cannot be finished with the context and tools, it will say it cannot answer\n",
    "If GPT picks a tool, execute the tool and save the result of the tool in memory.\n",
    "\"\"\"\n",
    "def parseResponse(response, memory,tools):\n",
    "    finished = False\n",
    "    print(\"---RESPONSE---\", response)\n",
    "    # \"USE : searchArxiv('ReAct reasoning and acting in language models')\"\n",
    "\n",
    "    if response.startswith('FINAL ANSWER:'):\n",
    "        finished = True\n",
    "        memory.append(response)\n",
    "        return (finished, response, memory)\n",
    "    elif response == 'FINAL: CANNOT ANSWER':\n",
    "        finished = True\n",
    "        memory.append(response)\n",
    "        return (finished, response, memory)\n",
    "    elif response.startswith('USE '):\n",
    "        # split the string using ':' as the delimiter\n",
    "        parsed_str = response.split(':')\n",
    "        print(\"parsed string\", parsed_str)\n",
    "        #['USE ', \" searchArxiv('React reasoning')\"]\n",
    "\n",
    "\n",
    "        # get the tool name and parameter\n",
    "        # tool_name = parsed_str[1].split(\"(\")[1]\n",
    "        # parameter = parsed_str[1]\n",
    "\n",
    "        tool_name = parsed_str[1].split(\"(\")[0].strip()\n",
    "        # tool_name = tool_name_with_extra.split(\"'\")[0].strip()\n",
    "\n",
    "        parameter_with_extra = parsed_str[1].split(\"(\")[1]\n",
    "        parameter = parameter_with_extra.split(\"'\")[1].strip()\n",
    "\n",
    "        print(\"Tool Name:\", tool_name)\n",
    "        print(\"Parameter:\", parameter)\n",
    "\n",
    "        print(\"THOUGHT: \" + response)\n",
    "        memory.append(\"THOUGHT: \" + response)\n",
    "\n",
    "        result = executeTool(tool_name, parameter,tools)\n",
    "\n",
    "        new_memory = \"OBSERVATION: \" + str(result)\n",
    "        print(new_memory)\n",
    "        memory.append(new_memory)\n",
    "\n",
    "        return (finished, result, memory)\n",
    "\n",
    "\"\"\"\n",
    "Execute the tool that GPT picks using the parameter it gives.\n",
    "Returns the execution result so that GPT can have the relevant info.\n",
    "\"\"\"\n",
    "def executeTool(tool_name, parameter,tools):\n",
    "    # Find the tool with the given name\n",
    "    tool = None\n",
    "    for t in tools:\n",
    "        if t['tool_name'] == tool_name:\n",
    "            tool = t\n",
    "            break\n",
    "    \n",
    "    # If the tool is found, execute its function with the given parameter\n",
    "    if tool:\n",
    "        return tool['function_name'](parameter)\n",
    "    else:\n",
    "        return \"Tool not found\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Wrap the search arxiv function as a tool for GPT\n",
    "Input is a search keyword\n",
    "Output is a list of dictionaries with title, published date, authors, and summary of papers\n",
    "\"\"\"\n",
    "def searchArxiv(keyword):\n",
    "    # Perform a search with the given query\n",
    "    search = arxiv.Search(query=keyword, max_results=3)\n",
    "    \n",
    "    # Get the metadata for each result and extract relevant information\n",
    "    results = []\n",
    "    for result in search.results():\n",
    "        title = result.title\n",
    "        published_date = result.published.strftime(\"%Y-%m-%d\")\n",
    "        authors = \", \".join(author.name for author in result.authors)\n",
    "        summary = result.summary\n",
    "        \n",
    "        # Store the extracted information as a dictionary\n",
    "        results.append((\n",
    "            \"title: \" + title,\n",
    "            \"published_date: \" + published_date,\n",
    "            \"authors: \" + authors,\n",
    "            \"summary: \" + summary\n",
    "        ))\n",
    "    \n",
    "    # Return the list of tuples containing the result information\n",
    "    return results\n",
    "\n",
    "\"\"\"\n",
    "Initialize memory, tools for the GPT agent.\n",
    "Ask for a user objective and let it run iteratively untill the objective is achieved.\n",
    "As a safety measure, it will also stop after 5 iterations just in case things go wrong.\n",
    "\"\"\"\n",
    "def startAgent():\n",
    "    objective = input(\"What is your research question? \")\n",
    "    # For simplicity, we will just use a list to store every thing. \n",
    "    # For production, you will probably use vector databases.\n",
    "    memory = []\n",
    "\n",
    "    tools = [{'tool_name': 'searchArxiv', \n",
    "            'description': \"\"\"You can use this tool to search for scientific papers on Arxiv. The response will have title, author, published date, and summary.\"\"\", \n",
    "            'function_name' : searchArxiv,\n",
    "            'parameter': 'search key word'}]\n",
    "    \n",
    "    n = 0\n",
    "    while True:\n",
    "        (finished, result, memory) = determineAction(objective, memory, tools)\n",
    "        n += 1\n",
    "\n",
    "        if finished:\n",
    "            print(result)\n",
    "            return\n",
    "        \n",
    "        if n > 5:\n",
    "            print(\"Ended for reaching limit.\")\n",
    "            return\n",
    "\n",
    "\n",
    "startAgent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* More reliable output using Instructor (https://jxnl.github.io/instructor/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

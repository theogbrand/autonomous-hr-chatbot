{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "babyAGI from scratch\n",
    "\n",
    "source:\n",
    "1. https://community.openai.com/t/building-agent-from-scratch/240703/6\n",
    "2. https://github.com/Troyanovsky/autonomous_agent_tutorial/blob/main/autonomous_agent_handson.ipynb\n",
    "  * https://bootcamp.uxdesign.cc/a-comprehensive-and-hands-on-guide-to-autonomous-agents-with-gpt-b58d54724d50\n",
    "\n",
    "TODO:\n",
    "1. Create/use an API replay tool. Make it easy to replay API calls as it’s too slow to copy & paste these into the OpenAI playground.\n",
    "2. Stream completions. Use the stream mode of the API to speed up dev cycles. You can quickly abort if a completion is off the rails.\n",
    "\n",
    "Extensions:\n",
    "* https://twitter.com/yoheinakajima/status/1666313838868992001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../creds/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "azure_endpoint = \"https://cursor-gpt-4.openai.azure.com\"\n",
    "api_version=\"2024-02-15-preview\"\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=api_version,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****OBJECTIVE*****\n",
      "Become a machine learning expert.\n",
      "****Expounding based on task:**** Learn about tensors.\n",
      "#### (1) Generated Task ####\n",
      "****Expounding based on task:**** Research and write a summary of the different data types that tensors can store in machine learning, including their significance and use cases.\n",
      "In machine learning, tensors are multi-dimensional arrays that can store various types of data. Tensors are the fundamental building blocks of most machine learning frameworks, and they can store different types of data, including scalar, vector, and matrix data.\n",
      "\n",
      "1. Scalar Data:\n",
      "Scalars are single numerical values, such as integers or floating-point numbers, and they are represented as tensors with zero dimensions. Scalar tensors are commonly used to store constants or individual data points in machine learning algorithms. For example, scalar tensors can be used to represent bias values in neural networks or as individual data points in statistical analysis.\n",
      "\n",
      "2. Vector Data:\n",
      "Vectors are one-dimensional arrays of numerical values and are represented as tensors with one dimension. In machine learning, vectors are commonly used to store feature vectors, which represent input data for training models. For example, in image recognition tasks, each image can be represented as a vector of pixel values stored in a tensor. Vectors are also used to represent output labels in classification tasks.\n",
      "\n",
      "3. Matrix Data:\n",
      "Matrices are two-dimensional arrays of numerical values and are represented as tensors with two dimensions. Matrices are widely used in machine learning for tasks such as linear algebra operations, image processing, and natural language processing. For example, in natural language processing, word embeddings are often represented as matrices stored in tensors, where each row corresponds to a word and each column corresponds to a feature dimension.\n",
      "\n",
      "4. Higher-Dimensional Data:\n",
      "Tensors can also store higher-dimensional data, such as three-dimensional or higher-dimensional arrays. These higher-dimensional tensors are commonly used in tasks such as image processing, video analysis, and time-series data. For example, in image processing, a color image can be represented as a three-dimensional tensor, where the dimensions correspond to the height, width, and color channels of the image.\n",
      "\n",
      "Overall, tensors in machine learning can store a wide variety of data types, including scalar, vector, matrix, and higher-dimensional data. Understanding the significance and use cases of these different data types is essential for effectively working with tensors in machine learning applications.\n",
      "#### (2) Generated Task ####\n",
      "****Expounding based on task:**** Create a Python script using TensorFlow or PyTorch to define and manipulate tensors of different ranks and shapes, and perform basic operations such as addition, multiplication, and matrix operations.\n",
      "Sure, I can help you with that. Below is an example of a Python script using TensorFlow to define and manipulate tensors of different ranks and shapes, and perform basic operations such as addition, multiplication, and matrix operations.\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "\n",
      "# Define tensors of different ranks and shapes\n",
      "scalar = tf.constant(5)  # a rank 0 tensor (or scalar)\n",
      "vector = tf.constant([1, 2, 3, 4])  # a rank 1 tensor (or vector)\n",
      "matrix = tf.constant([[1, 2], [3, 4]])  # a rank 2 tensor (or matrix)\n",
      "cube = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # a rank 3 tensor (or cube)\n",
      "\n",
      "# Perform basic operations\n",
      "addition = tf.add(matrix, matrix)  # element-wise addition\n",
      "multiplication = tf.multiply(matrix, matrix)  # element-wise multiplication\n",
      "matrix_product = tf.matmul(matrix, matrix)  # matrix multiplication\n",
      "\n",
      "# Print the results\n",
      "with tf.Session() as sess:\n",
      "    print(\"Scalar:\", sess.run(scalar))\n",
      "    print(\"Vector:\", sess.run(vector))\n",
      "    print(\"Matrix:\", sess.run(matrix))\n",
      "    print(\"Cube:\", sess.run(cube))\n",
      "    print(\"Matrix addition:\", sess.run(addition))\n",
      "    print(\"Matrix multiplication:\", sess.run(multiplication))\n",
      "    print(\"Matrix product:\", sess.run(matrix_product))\n",
      "```\n",
      "\n",
      "You can run this script to see how tensors of different ranks and shapes are defined and manipulated using TensorFlow, and how basic operations such as addition, multiplication, and matrix operations are performed.\n",
      "#### (3) Generated Task ####\n",
      "****Expounding based on task:**** Investigate and summarize the role of tensors in representing input data, model parameters, and output predictions in the context of machine learning, with a focus on how tensors are used in deep learning frameworks.\n",
      "Tensors play a fundamental role in representing input data, model parameters, and output predictions in the context of machine learning. In the context of deep learning frameworks, tensors are used extensively due to their ability to efficiently represent and manipulate multi-dimensional data.\n",
      "\n",
      "Input Data:\n",
      "In machine learning, input data such as images, sound waves, and text are represented as tensors. For example, an RGB image can be represented as a 3D tensor, with dimensions corresponding to height, width, and color channels. Similarly, a sequence of words in natural language processing can be represented as a 2D tensor, with dimensions corresponding to the length of the sequence and the embedding size of each word.\n",
      "\n",
      "Model Parameters:\n",
      "The parameters of a machine learning model, such as weights and biases, are also represented as tensors. These tensors capture the learned information from the input data and are updated during the training process to minimize the loss function.\n",
      "\n",
      "Output Predictions:\n",
      "The output predictions of a machine learning model are also represented as tensors. The dimensions of the output tensor depend on the specific task, such as classification, regression, or sequence generation.\n",
      "\n",
      "Deep Learning Frameworks:\n",
      "Deep learning frameworks, such as TensorFlow and PyTorch, provide efficient and optimized operations for manipulating tensors. These frameworks offer a wide range of tensor operations, including tensor multiplication, addition, convolution, and more. They also provide automatic differentiation, which is crucial for training deep learning models.\n",
      "\n",
      "In summary, tensors are a crucial concept in machine learning, especially in the context of deep learning. They are used to represent input data, model parameters, and output predictions, and deep learning frameworks provide powerful tools for working with tensors efficiently.\n",
      "#### (4) Generated Task ####\n",
      "****Expounding based on task:**** Explore and compare the advantages and disadvantages of using different tensor manipulation operations in machine learning, such as element-wise operations, matrix multiplication, and tensor reshaping.\n",
      "Tensor manipulation operations are crucial in machine learning as they allow us to perform various mathematical operations on the data. Here, I will explore and compare the advantages and disadvantages of using different tensor manipulation operations such as element-wise operations, matrix multiplication, and tensor reshaping.\n",
      "\n",
      "1. Element-wise operations:\n",
      "   Advantages:\n",
      "   - Element-wise operations are simple and efficient, making it easy to perform operations such as addition, subtraction, multiplication, and division on tensors.\n",
      "   - These operations can be parallelized, which can significantly speed up the calculations, especially on hardware with parallel processing capabilities like GPUs.\n",
      "   \n",
      "   Disadvantages:\n",
      "   - Element-wise operations may not fully utilize the computational power of modern hardware, as they do not fully exploit the potential of parallel processing.\n",
      "\n",
      "2. Matrix multiplication:\n",
      "   Advantages:\n",
      "   - Matrix multiplication is a fundamental operation in many machine learning algorithms, such as neural networks. It allows us to combine and transform data in complex ways.\n",
      "   - This operation can be optimized for parallel processing, making it well-suited for modern hardware with multiple cores or GPUs.\n",
      "\n",
      "   Disadvantages:\n",
      "   - Matrix multiplication can be computationally expensive, especially for large matrices, and may become a bottleneck in the performance of machine learning algorithms.\n",
      "   - It requires careful handling of dimensions and shapes to ensure compatibility, which can be error-prone.\n",
      "\n",
      "3. Tensor reshaping:\n",
      "   Advantages:\n",
      "   - Tensor reshaping allows us to reorganize the data without changing its contents, which is useful for preparing the input data for different layers in a neural network or for interfacing with different machine learning libraries.\n",
      "   - It can help in reducing the memory footprint of the data, making it more memory-efficient.\n",
      "\n",
      "   Disadvantages:\n",
      "   - Reshaping tensors requires careful consideration of the new shape and may lead to errors if not done correctly.\n",
      "   - Reshaping can also introduce complexity in the code and make it harder to understand and maintain.\n",
      "\n",
      "In conclusion, each tensor manipulation operation has its advantages and disadvantages. Element-wise operations are simple and efficient but may not fully exploit the computational power of modern hardware. Matrix multiplication is fundamental for many machine learning algorithms but can be computationally expensive. Tensor reshaping is useful for reorganizing data but requires careful handling of shapes and dimensions. It's important to carefully consider the trade-offs and choose the appropriate operations based on the specific requirements of the machine learning task at hand.\n",
      "#### (5) Generated Task ####\n",
      "****Expounding based on task:**** Develop a tutorial on how to implement and optimize a simple machine learning model using tensors in TensorFlow or PyTorch, focusing on the practical application of tensors in model training and inference processes.\n",
      "To implement and optimize a simple machine learning model using tensors in TensorFlow, you can follow the tutorial below. We will walk through the process of creating a simple neural network model, training it, and making predictions using TensorFlow and its high-level Keras API.\n",
      "\n",
      "Step 1: Install TensorFlow\n",
      "First, you need to install TensorFlow. You can do this using pip:\n",
      "\n",
      "```bash\n",
      "pip install tensorflow\n",
      "```\n",
      "\n",
      "Step 2: Import the necessary libraries\n",
      "Once TensorFlow is installed, you can import the necessary libraries in your Python script:\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "from tensorflow import keras\n",
      "import numpy as np\n",
      "```\n",
      "\n",
      "Step 3: Prepare the data\n",
      "Next, you need to prepare the training and testing data for your model. In this example, we'll use a simple dataset of input features and corresponding target labels:\n",
      "\n",
      "```python\n",
      "# Generate dummy data\n",
      "x_train = np.random.random((1000, 3))\n",
      "y_train = np.random.randint(2, size=(1000, 1))\n",
      "x_test = np.random.random((100, 3))\n",
      "y_test = np.random.randint(2, size=(100, 1))\n",
      "```\n",
      "\n",
      "Step 4: Create the model\n",
      "Now, you can create a simple neural network model using TensorFlow's Keras API. We'll define a model with a single hidden layer and an output layer:\n",
      "\n",
      "```python\n",
      "model = keras.Sequential([\n",
      "    keras.layers.Dense(64, input_shape=(3,), activation='relu'),\n",
      "    keras.layers.Dense(1, activation='sigmoid')\n",
      "])\n",
      "```\n",
      "\n",
      "Step 5: Compile the model\n",
      "After creating the model, you need to compile it by specifying the loss function, optimizer, and metrics to be used during training:\n",
      "\n",
      "```python\n",
      "model.compile(optimizer='adam',\n",
      "              loss='binary_crossentropy',\n",
      "              metrics=['accuracy'])\n",
      "```\n",
      "\n",
      "Step 6: Train the model\n",
      "Now, you can train the model using the prepared training data:\n",
      "\n",
      "```python\n",
      "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
      "```\n",
      "\n",
      "Step 7: Make predictions\n",
      "Once the model is trained, you can use it to make predictions on new data:\n",
      "\n",
      "```python\n",
      "predictions = model.predict(x_test)\n",
      "```\n",
      "\n",
      "Step 8: Model evaluation\n",
      "Finally, you can evaluate the model's performance on the test data:\n",
      "\n",
      "```python\n",
      "loss, accuracy = model.evaluate(x_test, y_test)\n",
      "print(f'Test accuracy: {accuracy}')\n",
      "```\n",
      "\n",
      "Optimizing the model:\n",
      "To optimize the model, you can experiment with different hyperparameters such as the number of layers, nodes per layer, learning rate, and batch size. Additionally, you can try different activation functions, regularization techniques, and other advanced optimization algorithms provided by TensorFlow.\n",
      "\n",
      "In conclusion, this tutorial has covered the practical implementation of a simple machine learning model using tensors in TensorFlow. By following these steps, you can gain hands-on experience in building and optimizing machine learning models using TensorFlow's powerful tensor operations and high-level Keras API.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import openai\n",
    "# Goal: Design a simple AI Agent with no dependencies!\n",
    "# This AI will NOT run forever.  It is also safe since it doesn't have API access beyond the OpenAI API.\n",
    "#\n",
    "# Usage: Just set your MainObjective, InitialTask, OPENAI_API_KEY at a minimum.\n",
    "#\n",
    "# Tips: Feel free to play with the temperature and run over and over for different answers.\n",
    "#\n",
    "# Inspired from BabyAGI: https://github.com/yoheinakajima/babyagi\n",
    "# BabyAGI has many more features and bells and whistles.  But may be hard to understand for beginners.\n",
    "\n",
    "# Goal configuration\n",
    "MainObjective = \"Become a machine learning expert.\" # overall objective\n",
    "InitialTask = \"Learn about tensors.\" # first task to research\n",
    "\n",
    "# Note: As expected, GPT-4 gives much deeper answers.  But turbo is selected here as the default, so as there no cost surprises.\n",
    "OPENAI_API_MODEL = \"pjf-dpo-turbo-35\" # use \"gpt-4\" or \"gpt-3.5-turbo\"\n",
    "# deployment_name = \"cursor-gpt-4\"\n",
    "# deployment_name = \"pjf-dpo-turbo-35\"\n",
    "\n",
    "# Model configuration\n",
    "OPENAI_TEMPERATURE = 0.7\n",
    "\n",
    "# Max tokens that the model can output per completion\n",
    "OPENAI_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "# print objective\n",
    "print(\"*****OBJECTIVE*****\")\n",
    "print(f\"{MainObjective}\")\n",
    "\n",
    "\n",
    "# dump task array to string\n",
    "def dumpTask(task):\n",
    "    d = \"\" # init\n",
    "    for tasklet in task:\n",
    "        d += f\"\\n{tasklet.get('task_name','')}\"\n",
    "    d = d.strip()\n",
    "    return d\n",
    "\n",
    "\n",
    "# inference using OpenAI API, with error throws and backoffs\n",
    "def OpenAiInference(\n",
    "    prompt: str,\n",
    "    model: str = OPENAI_API_MODEL,\n",
    "    temperature: float = OPENAI_TEMPERATURE,\n",
    "    max_tokens: int = 1024,\n",
    "):\n",
    "    while True:\n",
    "        try:\n",
    "            # Use chat completion API\n",
    "            response = \"NOTHING\"\n",
    "            messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except openai.error.RateLimitError:\n",
    "            print(\n",
    "                \"   *** The OpenAI API rate limit has been exceeded. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.Timeout:\n",
    "            print(\n",
    "                \"   *** OpenAI API timeout occured. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.APIError:\n",
    "            print(\n",
    "                \"   *** OpenAI API error occured. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.APIConnectionError:\n",
    "            print(\n",
    "                \"   *** OpenAI API connection error occured. Check your network settings, proxy configuration, SSL certificates, or firewall rules. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.InvalidRequestError:\n",
    "            print(\n",
    "                \"   *** OpenAI API invalid request. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.ServiceUnavailableError:\n",
    "            print(\n",
    "                \"   *** OpenAI API service unavailable. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        finally:\n",
    "            pass\n",
    "            # print(f\"Inference Response: {response}\")\n",
    "\n",
    "# expound on the main objective given a task\n",
    "def ExpoundTask(MainObjective: str, CurrentTask: str):\n",
    "\n",
    "    print(f\"****Expounding based on task:**** {CurrentTask}\")\n",
    "\n",
    "    prompt=(f\"You are an AI who performs one task based on the following objective: {MainObjective}\\n\"\n",
    "            f\"Your task: {CurrentTask}\\nResponse:\")\n",
    "\n",
    "\n",
    "    # print(\"################\")\n",
    "    # print(prompt)\n",
    "    response = OpenAiInference(prompt, OPENAI_API_MODEL, OPENAI_TEMPERATURE, OPENAI_MAX_TOKENS)\n",
    "    new_tasks = response.split(\"\\n\") if \"\\n\" in response else [response]\n",
    "    return [{\"task_name\": task_name} for task_name in new_tasks]\n",
    "\n",
    "\n",
    "\n",
    "# generate a bunch of tasks based on the main objective and the current task\n",
    "def GenerateTasks(MainObjective: str, TaskExpansion: str):\n",
    "    prompt=(f\"You are an AI who creates tasks based on the following MAIN OBJECTIVE: {MainObjective}\\n\"\n",
    "            f\"Create tasks pertaining directly to your previous research here:\\n\"\n",
    "            f\"{TaskExpansion}\\nResponse:\")\n",
    "    response = OpenAiInference(prompt, OPENAI_API_MODEL, OPENAI_TEMPERATURE, OPENAI_MAX_TOKENS)\n",
    "    new_tasks = response.split(\"\\n\") if \"\\n\" in response else [response]\n",
    "    task_list = [{\"task_name\": task_name} for task_name in new_tasks]\n",
    "    new_tasks_list = []\n",
    "    for task_item in task_list:\n",
    "        # print(task_item)\n",
    "        task_description = task_item.get(\"task_name\")\n",
    "        if task_description:\n",
    "            # print(task_description)\n",
    "            task_parts = task_description.strip().split(\".\", 1)\n",
    "            # print(task_parts)\n",
    "            if len(task_parts) == 2:\n",
    "                new_task = task_parts[1].strip()\n",
    "                new_tasks_list.append(new_task)\n",
    "\n",
    "    return new_tasks_list\n",
    "\n",
    "# Simple version here, just generate tasks based on the inital task and objective, then expound with GPT against the main objective and the newly generated tasks.\n",
    "q = ExpoundTask(MainObjective,InitialTask)\n",
    "ExpoundedInitialTask = dumpTask(q)\n",
    "\n",
    "q = GenerateTasks(MainObjective, ExpoundedInitialTask)\n",
    "\n",
    "TaskCounter = 0\n",
    "for Task in q:\n",
    "    TaskCounter += 1\n",
    "    print(f\"#### ({TaskCounter}) Generated Task ####\")\n",
    "    e = ExpoundTask(MainObjective,Task)\n",
    "    print(dumpTask(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser==6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: requests==2.31.0 in /Users/ogb/projects/cal/venv/lib/python3.10/site-packages (from arxiv) (2.31.0)\n",
      "Collecting sgmllib3k (from feedparser==6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/ogb/projects/cal/venv/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ogb/projects/cal/venv/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ogb/projects/cal/venv/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ogb/projects/cal/venv/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (2023.5.7)\n",
      "Downloading arxiv-2.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=5b95e83d7a3adb82fbc9bde5f30937b0f0b190dfab4b2853a7cd69788bdfd53b\n",
      "  Stored in directory: /Users/ogb/Library/Caches/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.0 feedparser-6.0.10 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RESPONSE--- USE : searchArxiv('Reflexion for verbal reinforcement learning')\n",
      "parsed string ['USE ', \" searchArxiv('Reflexion for verbal reinforcement learning')\"]\n",
      "Tool Name: searchArxiv\n",
      "Parameter: Reflexion for verbal reinforcement learning\n",
      "THOUGHT: USE : searchArxiv('Reflexion for verbal reinforcement learning')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qp/hpk75cps1fxdym0m5bmqczz00000gn/T/ipykernel_86129/1417250408.py:132: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBSERVATION: [('title: Reflexion: Language Agents with Verbal Reinforcement Learning', 'published_date: 2023-03-20', 'authors: Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao', 'summary: Large language models (LLMs) have been increasingly used to interact with\\nexternal environments (e.g., games, compilers, APIs) as goal-driven agents.\\nHowever, it remains challenging for these language agents to quickly and\\nefficiently learn from trial-and-error as traditional reinforcement learning\\nmethods require extensive training samples and expensive model fine-tuning. We\\npropose Reflexion, a novel framework to reinforce language agents not by\\nupdating weights, but instead through linguistic feedback. Concretely,\\nReflexion agents verbally reflect on task feedback signals, then maintain their\\nown reflective text in an episodic memory buffer to induce better\\ndecision-making in subsequent trials. Reflexion is flexible enough to\\nincorporate various types (scalar values or free-form language) and sources\\n(external or internally simulated) of feedback signals, and obtains significant\\nimprovements over a baseline agent across diverse tasks (sequential\\ndecision-making, coding, language reasoning). For example, Reflexion achieves a\\n91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous\\nstate-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis\\nstudies using different feedback signals, feedback incorporation methods, and\\nagent types, and provide insights into how they affect performance.'), ('title: Explanation of Reinforcement Learning Model in Dynamic Multi-Agent System', 'published_date: 2020-08-04', 'authors: Xinzhi Wang, Huao Li, Hui Zhang, Michael Lewis, Katia Sycara', 'summary: Recently, there has been increasing interest in transparency and\\ninterpretability in Deep Reinforcement Learning (DRL) systems. Verbal\\nexplanations, as the most natural way of communication in our daily life,\\ndeserve more attention, since they allow users to gain a better understanding\\nof the system which ultimately could lead to a high level of trust and smooth\\ncollaboration. This paper reports a novel work in generating verbal\\nexplanations for DRL behaviors agent. A rule-based model is designed to\\nconstruct explanations using a series of rules which are predefined with prior\\nknowledge. A learning model is then proposed to expand the implicit logic of\\ngenerating verbal explanation to general situations by employing rule-based\\nexplanations as training data. The learning model is shown to have better\\nflexibility and generalizability than the static rule-based model. The\\nperformance of both models is evaluated quantitatively through objective\\nmetrics. The results show that verbal explanation generated by both models\\nimprove subjective satisfaction of users towards the interpretability of DRL\\nsystems. Additionally, seven variants of the learning model are designed to\\nillustrate the contribution of input channels, attention mechanism, and\\nproposed encoder in improving the quality of verbal explanation.'), ('title: A MultiModal Social Robot Toward Personalized Emotion Interaction', 'published_date: 2021-10-08', 'authors: Baijun Xie, Chung Hyuk Park', 'summary: Human emotions are expressed through multiple modalities, including verbal\\nand non-verbal information. Moreover, the affective states of human users can\\nbe the indicator for the level of engagement and successful interaction,\\nsuitable for the robot to use as a rewarding factor to optimize robotic\\nbehaviors through interaction. This study demonstrates a multimodal human-robot\\ninteraction (HRI) framework with reinforcement learning to enhance the robotic\\ninteraction policy and personalize emotional interaction for a human user. The\\ngoal is to apply this framework in social scenarios that can let the robots\\ngenerate a more natural and engaging HRI framework.')]\n",
      "---RESPONSE--- FINAL ANSWER: Reflexion for verbal reinforcement learning is a novel framework designed to reinforce language agents not by updating weights, but instead through linguistic feedback. Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. This method is flexible enough to incorporate various types and sources of feedback signals. It has shown significant improvements over a baseline agent across diverse tasks such as sequential decision-making, coding, and language reasoning. For instance, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%.\n",
      "FINAL ANSWER: Reflexion for verbal reinforcement learning is a novel framework designed to reinforce language agents not by updating weights, but instead through linguistic feedback. Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. This method is flexible enough to incorporate various types and sources of feedback signals. It has shown significant improvements over a baseline agent across diverse tasks such as sequential decision-making, coding, and language reasoning. For instance, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import arxiv\n",
    "\n",
    "\"\"\"\n",
    "Wrap the OpenAI API call in this function\n",
    "\"\"\"\n",
    "def getResponse(prompt):\n",
    "    response =  client.chat.completions.create(\n",
    "            # model=\"pjf-dpo-turbo-35\",\n",
    "            model=\"cursor-gpt-4\",\n",
    "            temperature = 0, # We want consistent behavior, so we set a very low temperature\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You're a helpful assistant. Carefully follow the user's instructions.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # response = response['choices'][0]['message']['content']\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\"\"\"\n",
    "Use GPT to determine the action to take by giving it the objective, memory, and tools.\n",
    "If it think it has finished the objective, just give the answer.\n",
    "If it needs more info, it will pick the tool to get the relevant information based on the tool description.\n",
    "\"\"\"\n",
    "def determineAction(objective, memory, tools):\n",
    "    formattedPrompt = f\"\"\"Determine if the following memory is enough to answer\\n\n",
    "    the user's objective. Your past actions are stored in the memory for reference\\n\n",
    "    If it is enough, answer the question in the format: 'FINAL ANSWER: '. \\n\n",
    "    If the memory is not enough, you can use a tool in the available tools section\\n\n",
    "    to get more information. When using a tool you should use this format: \\n\n",
    "    'USE :'. If no tool can help you achieve the user's \\n\n",
    "    objective, then answer 'FINAL: CANNOT ANSWER'.\n",
    "\n",
    "    ```Objective\n",
    "    Answer: {objective}\n",
    "    ```\n",
    "\n",
    "    ```Memory\n",
    "    {memory}\n",
    "    ```\n",
    "\n",
    "    ```Available Tools\n",
    "    {tools}\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    response = getResponse(formattedPrompt)\n",
    "    (finished, result, memory) = parseResponse(response, memory,tools)\n",
    "    return (finished, result, memory)\n",
    "\n",
    "\"\"\"\n",
    "Parse the response from GPT to determine if the objective is finished.\n",
    "If it is finished, just give the final answer.\n",
    "If the objective cannot be finished with the context and tools, it will say it cannot answer\n",
    "If GPT picks a tool, execute the tool and save the result of the tool in memory.\n",
    "\"\"\"\n",
    "def parseResponse(response, memory,tools):\n",
    "    finished = False\n",
    "    print(\"---RESPONSE---\", response)\n",
    "    # \"USE : searchArxiv('ReAct reasoning and acting in language models')\"\n",
    "\n",
    "    if response.startswith('FINAL ANSWER:'):\n",
    "        finished = True\n",
    "        memory.append(response)\n",
    "        return (finished, response, memory)\n",
    "    elif response == 'FINAL: CANNOT ANSWER':\n",
    "        finished = True\n",
    "        memory.append(response)\n",
    "        return (finished, response, memory)\n",
    "    elif response.startswith('USE '):\n",
    "        # split the string using ':' as the delimiter\n",
    "        parsed_str = response.split(':')\n",
    "        print(\"parsed string\", parsed_str)\n",
    "        #['USE ', \" searchArxiv('React reasoning')\"]\n",
    "\n",
    "\n",
    "        # get the tool name and parameter\n",
    "        # tool_name = parsed_str[1].split(\"(\")[1]\n",
    "        # parameter = parsed_str[1]\n",
    "\n",
    "        tool_name = parsed_str[1].split(\"(\")[0].strip()\n",
    "        # tool_name = tool_name_with_extra.split(\"'\")[0].strip()\n",
    "\n",
    "        parameter_with_extra = parsed_str[1].split(\"(\")[1]\n",
    "        parameter = parameter_with_extra.split(\"'\")[1].strip()\n",
    "\n",
    "        print(\"Tool Name:\", tool_name)\n",
    "        print(\"Parameter:\", parameter)\n",
    "\n",
    "        print(\"THOUGHT: \" + response)\n",
    "        memory.append(\"THOUGHT: \" + response)\n",
    "\n",
    "        result = executeTool(tool_name, parameter,tools)\n",
    "\n",
    "        new_memory = \"OBSERVATION: \" + str(result)\n",
    "        print(new_memory)\n",
    "        memory.append(new_memory)\n",
    "\n",
    "        return (finished, result, memory)\n",
    "\n",
    "\"\"\"\n",
    "Execute the tool that GPT picks using the parameter it gives.\n",
    "Returns the execution result so that GPT can have the relevant info.\n",
    "\"\"\"\n",
    "def executeTool(tool_name, parameter,tools):\n",
    "    # Find the tool with the given name\n",
    "    tool = None\n",
    "    for t in tools:\n",
    "        if t['tool_name'] == tool_name:\n",
    "            tool = t\n",
    "            break\n",
    "    \n",
    "    # If the tool is found, execute its function with the given parameter\n",
    "    if tool:\n",
    "        return tool['function_name'](parameter)\n",
    "    else:\n",
    "        return \"Tool not found\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Wrap the search arxiv function as a tool for GPT\n",
    "Input is a search keyword\n",
    "Output is a list of dictionaries with title, published date, authors, and summary of papers\n",
    "\"\"\"\n",
    "def searchArxiv(keyword):\n",
    "    # Perform a search with the given query\n",
    "    search = arxiv.Search(query=keyword, max_results=3)\n",
    "    \n",
    "    # Get the metadata for each result and extract relevant information\n",
    "    results = []\n",
    "    for result in search.results():\n",
    "        title = result.title\n",
    "        published_date = result.published.strftime(\"%Y-%m-%d\")\n",
    "        authors = \", \".join(author.name for author in result.authors)\n",
    "        summary = result.summary\n",
    "        \n",
    "        # Store the extracted information as a dictionary\n",
    "        results.append((\n",
    "            \"title: \" + title,\n",
    "            \"published_date: \" + published_date,\n",
    "            \"authors: \" + authors,\n",
    "            \"summary: \" + summary\n",
    "        ))\n",
    "    \n",
    "    # Return the list of tuples containing the result information\n",
    "    return results\n",
    "\n",
    "\"\"\"\n",
    "Initialize memory, tools for the GPT agent.\n",
    "Ask for a user objective and let it run iteratively untill the objective is achieved.\n",
    "As a safety measure, it will also stop after 5 iterations just in case things go wrong.\n",
    "\"\"\"\n",
    "def startAgent():\n",
    "    objective = input(\"What is your research question? \")\n",
    "    # For simplicity, we will just use a list to store every thing. \n",
    "    # For production, you will probably use vector databases.\n",
    "    memory = []\n",
    "\n",
    "    tools = [{'tool_name': 'searchArxiv', \n",
    "            'description': \"\"\"You can use this tool to search for scientific papers on Arxiv. The response will have title, author, published date, and summary.\"\"\", \n",
    "            'function_name' : searchArxiv,\n",
    "            'parameter': 'search key word'}]\n",
    "    \n",
    "    n = 0\n",
    "    while True:\n",
    "        (finished, result, memory) = determineAction(objective, memory, tools)\n",
    "        n += 1\n",
    "\n",
    "        if finished:\n",
    "            print(result)\n",
    "            return\n",
    "        \n",
    "        if n > 5:\n",
    "            print(\"Ended for reaching limit.\")\n",
    "            return\n",
    "\n",
    "\n",
    "startAgent()\n",
    "# try: what is Reflexion for verbal reinforcement learning paper about?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* More reliable output using Instructor (https://jxnl.github.io/instructor/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

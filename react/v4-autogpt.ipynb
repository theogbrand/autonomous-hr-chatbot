{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoGPT Prompt Style\n",
    "* https://dev.to/airtai/long-read-deep-dive-into-autogpt-a-comprehensive-and-in-depth-step-by-step-guide-to-how-it-works-48gd\n",
    "\n",
    "Feature:\n",
    "* look for articles that look like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../creds/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "azure_endpoint = \"https://cursor-gpt-4.openai.azure.com\"\n",
    "api_version=\"2024-02-15-preview\"\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=api_version,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "# Author: Curt Kennedy. \n",
    "# Date: Apr 22, 2023.\n",
    "# License: MIT\n",
    "# Goal: Design a simple AI Agent with no dependencies!\n",
    "# This AI will NOT run forever.  It is also safe since it doesn't have API access beyond the OpenAI API.\n",
    "#\n",
    "# Usage: Just set your MainObjective, InitialTask, OPENAI_API_KEY at a minimum.\n",
    "#\n",
    "# Tips: Feel free to play with the temperature and run over and over for different answers.\n",
    "#\n",
    "# Inspired from BabyAGI: https://github.com/yoheinakajima/babyagi\n",
    "# BabyAGI has many more features and bells and whistles.  But may be hard to understand for beginners.\n",
    "\n",
    "# Goal configuration\n",
    "MainObjective = \"Become a machine learning expert.\" # overall objective\n",
    "InitialTask = \"Learn about tensors.\" # first task to research\n",
    "\n",
    "# API Key\n",
    "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY_GOES_HERE\"\n",
    "\n",
    "# Note: As expected, GPT-4 gives much deeper answers.  But turbo is selected here as the default, so as there no cost surprises.\n",
    "OPENAI_API_MODEL = \"gpt-3.5-turbo\" # use \"gpt-4\" or \"gpt-3.5-turbo\"\n",
    "\n",
    "# Model configuration\n",
    "OPENAI_TEMPERATURE = 0.7\n",
    "\n",
    "# Max tokens that the model can output per completion\n",
    "OPENAI_MAX_TOKENS = 1024\n",
    "\n",
    "# init OpenAI Python SDK\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "# print objective\n",
    "print(\"*****OBJECTIVE*****\")\n",
    "print(f\"{MainObjective}\")\n",
    "\n",
    "\n",
    "# dump task array to string\n",
    "def dumpTask(task):\n",
    "    d = \"\" # init\n",
    "    for tasklet in task:\n",
    "        d += f\"\\n{tasklet.get('task_name','')}\"\n",
    "    d = d.strip()\n",
    "    return d\n",
    "\n",
    "\n",
    "# inference using OpenAI API, with error throws and backoffs\n",
    "def OpenAiInference(\n",
    "    prompt: str,\n",
    "    model: str = OPENAI_API_MODEL,\n",
    "    temperature: float = OPENAI_TEMPERATURE,\n",
    "    max_tokens: int = 1024,\n",
    "):\n",
    "    while True:\n",
    "        try:\n",
    "            # Use chat completion API\n",
    "            response = \"NOTHING\"\n",
    "            messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except openai.error.RateLimitError:\n",
    "            print(\n",
    "                \"   *** The OpenAI API rate limit has been exceeded. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.Timeout:\n",
    "            print(\n",
    "                \"   *** OpenAI API timeout occured. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.APIError:\n",
    "            print(\n",
    "                \"   *** OpenAI API error occured. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.APIConnectionError:\n",
    "            print(\n",
    "                \"   *** OpenAI API connection error occured. Check your network settings, proxy configuration, SSL certificates, or firewall rules. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.InvalidRequestError:\n",
    "            print(\n",
    "                \"   *** OpenAI API invalid request. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.ServiceUnavailableError:\n",
    "            print(\n",
    "                \"   *** OpenAI API service unavailable. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        finally:\n",
    "            pass\n",
    "            # print(f\"Inference Response: {response}\")\n",
    "\n",
    "# expound on the main objective given a task\n",
    "def ExpoundTask(MainObjective: str, CurrentTask: str):\n",
    "\n",
    "    print(f\"****Expounding based on task:**** {CurrentTask}\")\n",
    "\n",
    "    prompt=(f\"You are an AI who performs one task based on the following objective: {MainObjective}\\n\"\n",
    "            f\"Your task: {CurrentTask}\\nResponse:\")\n",
    "\n",
    "\n",
    "    # print(\"################\")\n",
    "    # print(prompt)\n",
    "    response = OpenAiInference(prompt, OPENAI_API_MODEL, OPENAI_TEMPERATURE, OPENAI_MAX_TOKENS)\n",
    "    new_tasks = response.split(\"\\n\") if \"\\n\" in response else [response]\n",
    "    return [{\"task_name\": task_name} for task_name in new_tasks]\n",
    "\n",
    "\n",
    "\n",
    "# generate a bunch of tasks based on the main objective and the current task\n",
    "def GenerateTasks(MainObjective: str, TaskExpansion: str):\n",
    "    prompt=(f\"You are an AI who creates tasks based on the following MAIN OBJECTIVE: {MainObjective}\\n\"\n",
    "            f\"Create tasks pertaining directly to your previous research here:\\n\"\n",
    "            f\"{TaskExpansion}\\nResponse:\")\n",
    "    response = OpenAiInference(prompt, OPENAI_API_MODEL, OPENAI_TEMPERATURE, OPENAI_MAX_TOKENS)\n",
    "    new_tasks = response.split(\"\\n\") if \"\\n\" in response else [response]\n",
    "    task_list = [{\"task_name\": task_name} for task_name in new_tasks]\n",
    "    new_tasks_list = []\n",
    "    for task_item in task_list:\n",
    "        # print(task_item)\n",
    "        task_description = task_item.get(\"task_name\")\n",
    "        if task_description:\n",
    "            # print(task_description)\n",
    "            task_parts = task_description.strip().split(\".\", 1)\n",
    "            # print(task_parts)\n",
    "            if len(task_parts) == 2:\n",
    "                new_task = task_parts[1].strip()\n",
    "                new_tasks_list.append(new_task)\n",
    "\n",
    "    return new_tasks_list\n",
    "\n",
    "# Simple version here, just generate tasks based on the inital task and objective, then expound with GPT against the main objective and the newly generated tasks.\n",
    "q = ExpoundTask(MainObjective,InitialTask)\n",
    "ExpoundedInitialTask = dumpTask(q)\n",
    "\n",
    "q = GenerateTasks(MainObjective, ExpoundedInitialTask)\n",
    "\n",
    "TaskCounter = 0\n",
    "for Task in q:\n",
    "    TaskCounter += 1\n",
    "    print(f\"#### ({TaskCounter}) Generated Task ####\")\n",
    "    e = ExpoundTask(MainObjective,Task)\n",
    "    print(dumpTask(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THREE types of tools: Code Interpreter, Retrieval, Function Calling\n",
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8vFihbxIPrB8ib9zfd69rt870LoiJ', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_L8SgNpYAuYUI3LzPQiG307GF', function=Function(arguments='{\"location\":\"Seattle\",\"unit\":\"celsius\"}', name='get_current_weather'), type='function')]), content_filter_results={})], created=1708655815, model='gpt-35-turbo', object='chat.completion', system_fingerprint='fp_68a7d165bf', usage=CompletionUsage(completion_tokens=20, prompt_tokens=105, total_tokens=125), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like today in Seattle?\"}\n",
    "]\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"pjf-dpo-turbo-35\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",  # auto is default, or none\n",
    ")\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "Always look things up on Wikipedia if you have the opportunity to do so.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What is the capital of France?\n",
    "Thought: I should look up France on Wikipedia\n",
    "Action: wikipedia: France\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: France is a country. The capital is Paris.\n",
    "\n",
    "You then output:\n",
    "\n",
    "Answer: The capital of France is Paris\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "    \n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "    \n",
    "    def execute(self):\n",
    "        # deployment_name = \"cursor-gpt-4\"\n",
    "        deployment_name = \"pjf-dpo-turbo-35\"\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment_name,  # e.g. gpt-35-instant\n",
    "            messages=self.messages,\n",
    "            # tools=functions,\n",
    "        )\n",
    "        # Uncomment this to print out token usage each time, e.g.\n",
    "        # {\"completion_tokens\": 86, \"prompt_tokens\": 26, \"total_tokens\": 112}\n",
    "        # print(completion.usage)\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wikipedia(q):\n",
    "#     return httpx.get(\"https://en.wikipedia.org/w/api.php\", params={\n",
    "#         \"action\": \"query\",\n",
    "#         \"list\": \"search\",\n",
    "#         \"srsearch\": q,\n",
    "#         \"format\": \"json\"\n",
    "#     }).json()[\"query\"][\"search\"][0][\"snippet\"]\n",
    "\n",
    "\n",
    "# def simon_blog_search(q):\n",
    "#     results = httpx.get(\"https://datasette.simonwillison.net/simonwillisonblog.json\", params={\n",
    "#         \"sql\": \"\"\"\n",
    "#         select\n",
    "#           blog_entry.title || ': ' || substr(html_strip_tags(blog_entry.body), 0, 1000) as text,\n",
    "#           blog_entry.created\n",
    "#         from\n",
    "#           blog_entry join blog_entry_fts on blog_entry.rowid = blog_entry_fts.rowid\n",
    "#         where\n",
    "#           blog_entry_fts match escape_fts(:q)\n",
    "#         order by\n",
    "#           blog_entry_fts.rank\n",
    "#         limit\n",
    "#           1\"\"\".strip(),\n",
    "#         \"_shape\": \"array\",\n",
    "#         \"q\": q,\n",
    "#     }).json()\n",
    "#     return results[0][\"text\"]\n",
    "\n",
    "def calculate(what):\n",
    "    return eval(what)\n",
    "\n",
    "known_actions = {\n",
    "    # \"wikipedia\": wikipedia,\n",
    "    \"calculate\": calculate,\n",
    "    # \"simon_blog_search\": simon_blog_search\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "action_re = re.compile('^Action: (\\w+): (.*)$')\n",
    "# action_input_regex = re.compile('^Action Input: (\\w+): (.*)$')\n",
    "\n",
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    bot = Agent(react_prompt)\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = bot(next_prompt)\n",
    "        print(result)\n",
    "        actions = [action_re.match(a) for a in result.split('\\n') if action_re.match(a)]\n",
    "        print(\"Actions Retrieved\", actions)\n",
    "        # action_inputs = [action_input_regex.match(a) for a in result.split('\\n') if action_input_regex.match(a)]\n",
    "        # print(\"Action Inputs\", action_inputs)\n",
    "        if actions:\n",
    "            # There is an action to run\n",
    "            action, action_input = actions[0].groups()\n",
    "            if action not in known_actions:\n",
    "                raise Exception(\"Unknown action: {}: {}\".format(action, action_input))\n",
    "            print(\" -- running {} {}\".format(action, action_input))\n",
    "            observation = known_actions[action](action_input)\n",
    "            print(\"Observation:\", observation)\n",
    "            next_prompt = \"Observation: {}\".format(observation)\n",
    "        else:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"what is 2 * twenty five\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* ask user for clarification after single ReAct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

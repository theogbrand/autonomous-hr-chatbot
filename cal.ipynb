{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-azure-openai\n",
    "%pip install llama-index-llms-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index chromadb --quiet\n",
    "%pip install chromadb\n",
    "%pip install sentence-transformers\n",
    "%pip install pydantic==1.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ogb/projects/cal/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "# # MSFT\n",
    "openai_api_key = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/pjf-dpo-turbo-35/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/pjf-dpo-turbo-35/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on the programming language Lisp. Graham has also written influential essays on startups, technology, and entrepreneurship.\n"
     ]
    }
   ],
   "source": [
    "api_key = openai_api_key\n",
    "azure_endpoint = \"https://cursor-gpt-4.openai.azure.com\"\n",
    "api_version=\"2024-02-15-preview\"\n",
    "\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"pjf-dpo-turbo-35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "response = llm.complete(\"Paul Graham is \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"ada_gcal\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 7dad6038-ea0e-4ec1-b7e3-693be983f663\n"
     ]
    }
   ],
   "source": [
    "# import pymongo\n",
    "# from llama_index.vector_stores.azurecosmosmongo import (\n",
    "#     AzureCosmosDBMongoDBVectorSearch,\n",
    "# )\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "# from llama_index.core import StorageContext\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "print(\"Document ID:\", documents[0].doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-chroma\n",
    "%pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# global\n",
    "# Settings.embed_model = OpenAIEmbedding()\n",
    "\n",
    "# per-index\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/pjf-dpo-turbo-35/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/pjf-dpo-turbo-35/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "> Source (Doc id: d7361350-0899-49ee-953a-7e910912fb70): What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of schoo...\n",
      "\n",
      "> Source (Doc id: 1404bdfa-51b3-4917-b382-f70e275437e2): Now that I could write essays again, I wrote a bunch about topics I'd had stacked up. I kept writ...\n",
      "response was:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on the IBM 1401 using an early version of Fortran. Later, with the advent of microcomputers, the author started programming more extensively, writing simple games, a program to predict rocket heights, and a word processor.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "print(response.get_formatted_sources())\n",
    "\n",
    "print(\"response was:\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "\n",
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "\n",
    "# define embedding function\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# load documents\n",
    "# documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Query Data\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

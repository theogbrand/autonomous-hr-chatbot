{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-azure-openai\n",
    "%pip install llama-index-llms-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index chromadb --quiet\n",
    "%pip install chromadb\n",
    "%pip install sentence-transformers\n",
    "%pip install pydantic==1.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ogb/projects/cal/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "# # MSFT\n",
    "openai_api_key = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/pjf-dpo-turbo-35/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/pjf-dpo-turbo-35/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on the programming language Lisp. Graham has also written influential essays on startups, technology, and entrepreneurship.\n"
     ]
    }
   ],
   "source": [
    "api_key = openai_api_key\n",
    "azure_endpoint = \"https://cursor-gpt-4.openai.azure.com\"\n",
    "api_version=\"2024-02-15-preview\"\n",
    "\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"pjf-dpo-turbo-35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "response = llm.complete(\"Paul Graham is \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"ada_gcal\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 7dad6038-ea0e-4ec1-b7e3-693be983f663\n"
     ]
    }
   ],
   "source": [
    "# import pymongo\n",
    "# from llama_index.vector_stores.azurecosmosmongo import (\n",
    "#     AzureCosmosDBMongoDBVectorSearch,\n",
    "# )\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "# from llama_index.core import StorageContext\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "print(\"Document ID:\", documents[0].doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-chroma\n",
    "%pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# global\n",
    "# Settings.embed_model = OpenAIEmbedding()\n",
    "\n",
    "# per-index\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/pjf-dpo-turbo-35/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/pjf-dpo-turbo-35/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "> Source (Doc id: d7361350-0899-49ee-953a-7e910912fb70): What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of schoo...\n",
      "\n",
      "> Source (Doc id: 1404bdfa-51b3-4917-b382-f70e275437e2): Now that I could write essays again, I wrote a bunch about topics I'd had stacked up. I kept writ...\n",
      "response was:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on the IBM 1401 using an early version of Fortran. Later, with the advent of microcomputers, the author started programming more extensively, writing simple games, a program to predict rocket heights, and a word processor.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "print(response.get_formatted_sources())\n",
    "\n",
    "print(\"response was:\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "UniqueConstraintError",
     "evalue": "Collection quickstart already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUniqueConstraintError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# create client and a new collection\u001b[39;00m\n\u001b[1;32m      9\u001b[0m chroma_client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mEphemeralClient()\n\u001b[0;32m---> 10\u001b[0m chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[43mchroma_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquickstart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# define embedding function\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# set up ChromaVectorStore and load in data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m ChromaVectorStore(chroma_collection\u001b[38;5;241m=\u001b[39mchroma_collection)\n",
      "File \u001b[0;32m~/projects/cal/venv/lib/python3.10/site-packages/chromadb/api/client.py:198\u001b[0m, in \u001b[0;36mClient.create_collection\u001b[0;34m(self, name, metadata, embedding_function, data_loader, get_or_create)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_collection\u001b[39m(\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     get_or_create: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/cal/venv/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/cal/venv/lib/python3.10/site-packages/chromadb/api/segment.py:166\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[0;34m(self, name, metadata, embedding_function, data_loader, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    162\u001b[0m check_index_name(name)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m uuid4()\n\u001b[0;32m--> 166\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sysdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n\u001b[1;32m    177\u001b[0m     segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mcreate_segments(coll)\n",
      "File \u001b[0;32m~/projects/cal/venv/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/cal/venv/lib/python3.10/site-packages/chromadb/db/mixins/sysdb.py:225\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[0;34m(self, id, name, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_collections(\n\u001b[1;32m    220\u001b[0m                 \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mcollection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase\n\u001b[1;32m    221\u001b[0m             )[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    223\u001b[0m         )\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UniqueConstraintError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assignment_policy\u001b[38;5;241m.\u001b[39massign_collection(\u001b[38;5;28mid\u001b[39m)\n\u001b[1;32m    228\u001b[0m collection \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[1;32m    230\u001b[0m     topic\u001b[38;5;241m=\u001b[39mtopic,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     database\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[1;32m    236\u001b[0m )\n",
      "\u001b[0;31mUniqueConstraintError\u001b[0m: Collection quickstart already exists"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "\n",
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "\n",
    "# define embedding function\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# load documents\n",
    "# documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Query Data\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/ada_gcal/embeddings?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/pjf-dpo-turbo-35/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://cursor-gpt-4.openai.azure.com/openai/deployments/pjf-dpo-turbo-35/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>The author enjoyed programming and writing, particularly during their early years before college. They found joy in writing short stories and programming on the IBM 1401, as well as later on microcomputers like the TRS-80.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What did the author enjoy?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>> Source (Doc id: 2f780940-5312-437b-aa84-6edbcbd2f7bc): Now that I could write essays again, I wrote a bunch about topics I'd had stacked up. I kept writ...\n",
       "\n",
       "> Source (Doc id: c4ba9e98-3cb7-4496-b4bf-769850d4945c): What I Worked On\n",
       "\n",
       "February 2021\n",
       "\n",
       "Before college the two main things I worked on, outside of schoo...</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response.get_formatted_sources()}</b>\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
